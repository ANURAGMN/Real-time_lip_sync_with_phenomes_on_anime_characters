Real-time lip sync with phenomes on anime characters:

While solutions like Wave2Lip, SadTalker, TalkingHead, and others exist, they come with the limitation of taking more time.  
Until the technology matures, we have no option but to rely on other hackish ways of implementation. 
One such approach is to use rule-based phoneme-to-viseme mapping. This method, while not as sophisticated as AI-driven solutions, is fast and efficient for real-time applications.

How it Works
This "hackish" approach involves:

Breaking down spoken words into their base sounds, or phonemes (e.g., the word "chat" has the phonemes /ch/, /a/, and /t/).
Mapping each of these phonemes to a corresponding visual mouth shape, called a viseme. For example, the phonemes for "b," "p," and "m" all correspond to a single mouth shape where the lips are closed.

Animating a 2D or 3D character's mouth to display these visemes in sequence, timed to the speech.
This video provides an excellent introduction to the fundamentals of lip-sync animation and the various mouth shapes used in the process. An introduction to lip sync (and mouth shapes) - 
Animation Theory https://www.youtube.com/watch?app=desktop&v=h7U2YvmIMOo&ab_channel=DevonKong


